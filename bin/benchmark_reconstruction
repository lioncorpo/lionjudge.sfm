#!/usr/bin/env python

import argparse
import datetime
import logging
import numpy as np
import os.path
import shutil
import yaml

from opensfm import bow
from opensfm import commands
from opensfm import dataset
from opensfm import features
from opensfm import log
from opensfm import io
from opensfm import tracking


logger = logging.getLogger(__name__)
log.setup()


def load_config(work_dir):
    config = {}
    filepath = os.path.join(work_dir, 'config.yaml')
    if os.path.isfile(filepath):
        with open(filepath) as fin:
            new_config = yaml.safe_load(fin)
        if new_config:
            for k, v in new_config.items():
                config[k] = v

    return config


def save_config(config, work_dir):
    with open(os.path.join(work_dir, 'config.yaml'), 'w') as fout:
        yaml.dump(config, fout, default_flow_style=False)


def overwrite_config(config, work_dir):
    orig_config = load_config(work_dir)

    for key, value in config.items():
        orig_config[key] = value

    save_config(orig_config, work_dir)


def create_bench_dir(dataset):
    base_dir = os.path.dirname(os.path.normpath(dataset)) if \
        os.path.isdir(dataset) else \
        os.path.dirname(dataset)

    dataset_dir = os.path.basename(os.path.normpath(dataset))

    bench_dir = os.path.join(
        base_dir,
        dataset_dir + \
        '__bench_feat_match__' + \
        datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))

    io.mkdir_p(bench_dir)

    return bench_dir


def create_args(work_dir):
    parser = argparse.ArgumentParser()
    parser.add_argument('dataset')
    args = parser.parse_args([work_dir])

    return args


def create_dataset(orig_dir, bench_dir, config):
    if config['matcher_type'] == 'FLANN':
        return create_flann_dataset(orig_dir, bench_dir, config)
    if config['matcher_type'] == 'FLANN_COM':
        return create_flann_com_dataset(orig_dir, bench_dir, config)
    elif config['matcher_type'] in ['WORDS', 'WORDS_SYMMETRIC']:
        return create_words_dataset(orig_dir, bench_dir, config)
    elif config['matcher_type'] == 'MATRIX':
        return create_matrix_dataset(orig_dir, bench_dir, config)
    else:
        raise ValueError('Invalid matcher_type: {}'.format(
            config['matcher_type']))


def create_flann_dataset(orig_dir, bench_dir, config):
    work_dir = os.path.join(bench_dir, 'flann')
    shutil.copytree(orig_dir, work_dir)

    overwrite_config(config, work_dir)

    parser = argparse.ArgumentParser()
    parser.add_argument('dataset')
    args = parser.parse_args([work_dir])

    data = dataset.DataSet(args.dataset)

    for image in data.images():
        p, f, c = data.load_features(image)
        index = features.build_flann_index(f, data.config)
        data.save_feature_index(image, index)

    return data


def create_flann_com_dataset(orig_dir, bench_dir, config):
    work_dir = os.path.join(bench_dir, 'flann_com')
    shutil.copytree(orig_dir, work_dir)

    overwrite_config(config, work_dir)

    parser = argparse.ArgumentParser()
    parser.add_argument('dataset')
    args = parser.parse_args([work_dir])

    return dataset.DataSet(args.dataset)


def create_words_dataset(orig_dir, bench_dir, config):
    work_dir = os.path.join(bench_dir, 'words')
    shutil.copytree(orig_dir, work_dir)

    overwrite_config(config, work_dir)

    parser = argparse.ArgumentParser()
    parser.add_argument('dataset')
    args = parser.parse_args([work_dir])

    data = dataset.DataSet(args.dataset)

    for image in data.images():
        bows = bow.load_bows(data.config)
        n_closest = data.config['bow_words_to_match']
        p, f, c = data.load_features(image)
        closest_words = bows.map_to_words(
            f, n_closest, data.config['bow_matcher_type'])
        data.save_words(image, closest_words)

    return data


def create_matrix_dataset(orig_dir, bench_dir, config):
    work_dir = os.path.join(bench_dir, 'matrix')
    shutil.copytree(orig_dir, work_dir)

    overwrite_config(config, work_dir)

    args = create_args(work_dir)
    data = dataset.DataSet(args.dataset)

    for image in data.images():
        p, f, c = data.load_features(image)
        index = features.build_flann_index(f, data.config)
        data.save_feature_index(image, index)

    return data


def merge_dicts(x, y):
    z = x.copy()
    z.update(y)
    return z


def benchmark(data, config):
    features, colors = tracking.load_features(data, data.images())
    matches = tracking.load_matches(data,
                                    data.images(),
                                    'initial_matches.pkl.gz')
    g = tracking.create_tracks_graph(features,
                                     colors,
                                     matches,
                                     data.config)

    tracks_2 = set(n for n,d in g.nodes(data=True) if d['bipartite']==1)
    tracks_3 = [t for t in tracks_2 if len(g.neighbors(t)) > 2]

    score = 0
    points_2 = 0
    points_3 = 0
    re = []
    re_pixel = []

    graph = data.load_tracks_graph()
    rs = data.load_reconstruction()

    if len(rs) >= 1:
        r = rs[0]
        points_2 += len(r.points)
        for p in r.points:
            score += len(graph.neighbors(p)) - 2

            if len(graph.neighbors(p)) > 2:
                points_3 += 1

        exif = {}
        for shot in r.shots:
            exif[shot] = data.load_exif(shot)

        for p in r.points:
            if len(graph.neighbors(p)) < 3:
                continue

            for im in graph.neighbors(p):
                if im not in r.shots:
                    continue

                edge = graph.get_edge_data(p, im)

                shot = r.shots[im]
                point = r.points[p]
                reprojection = shot.project(point.coordinates)
                error = np.subtract(reprojection, edge['feature'])
                scale = np.max([exif[im]['width'], exif[im]['height']])
                error_pixel = scale * error

                re.append(error)
                re_pixel.append(error_pixel)

    return {
        'score': float(score) / len(data.images()),
        'reprojection_error': np.mean(np.linalg.norm(re, axis=1)),
        'reprojection_error_pixel': np.mean(np.linalg.norm(re_pixel, axis=1)),
        'tracks_2': 0 if not len(tracks_2) else float(points_2) / len(tracks_2),
        'tracks_3': 0 if not len(tracks_3) else float(points_3) / len(tracks_3),
        'points_2': points_2,
        'points_3': points_3,
    }


def report(data, config):
    run_definition = 'Config: '
    for key, value in config.items():
        run_definition += key + ': ' + str(value) + ', '

    logger.info('========================================')
    logger.info('Dataset: {}'.format(data.data_path))
    logger.info(run_definition)
    logger.info('Score: {}'.format(score))
    logger.info('Reconstructed tracks (length >= 2): {}'.format(float(points_2) / len(tracks_2)))
    logger.info('Reconstructed tracks (length >= 3): {}'.format(float(points_3) / len(tracks_3)))
    logger.info('Reconstructed points (length >= 2): {}'.format(points_2))
    logger.info('Reconstructed points (length >= 3): {}'.format(points_3))
    logger.info('========================================')


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Benchmark a reconstruction (exif data must exist)')
    parser.add_argument('--dataset', help='path to the dataset to be processed')

    args = parser.parse_args()

    lowes_ratio = 0.8

    datasets = [args.dataset] if args.dataset else \
        [
            'data/benchmark_run/aizu1',
            'data/benchmark_run/aizu2',
            'data/benchmark_run/brussels1',
            'data/benchmark_run/cph1',
            'data/benchmark_run/geelong1',
            'data/benchmark_run/hertogenbosch1',
            'data/benchmark_run/london1',
            'data/benchmark_run/portland1',
            'data/benchmark_run/sf1',
            'data/benchmark_run/vancouver1',
        ]

    default_config = {
        'processes': 1,
        'save_initial_matches': True,
        'lowes_ratio': lowes_ratio,
        'feature_use_adaptive_suppression': False,
        'feature_process_size': 2048,
        'matching_bow_neighbors': 0,
        'matching_gps_distance': 0,
        'matching_gps_neighbors': 0,
        'matching_order_neighbors': 0,
        'matching_time_neighbors': 0,
        'matching_bow_neighbors': 0,
        'matching_bow_gps_distance': 0,
        'matching_bow_gps_neighbors': 0,
        'matching_vlad_neighbors': 0,
        'matching_vlad_gps_distance': 0,
        'matching_vlad_gps_neighbors': 0,
    }

    configs = [
        merge_dicts(default_config,
        { 'matcher_type': 'MATRIX', 'matching_gps_neighbors': 100 }),
        merge_dicts(default_config,
        { 'matcher_type': 'WORDS_SYMMETRIC', 'bow_num_checks': 20, 'matching_bow_neighbors': 15 }),
        merge_dicts(default_config,
        { 'matcher_type': 'FLANN_COM', 'flann_iterations': 10, 'flann_branching': 8, 'flann_checks': 20, 'matching_vlad_neighbors': 15 }),
    ]

    for d in datasets:
        args.dataset = d
        overwrite_config(merge_dicts(default_config, { 'matcher_type': 'MATRIX' }), d)

        commands.detect_features.Command().run(args)

    bench_results = []
    for c in configs:
        bench_result = {
            "config": c,
            "datasets": [],
            "results": [],
        }

        for d in datasets:
            bench_dir = create_bench_dir(d)
            bench_data = create_dataset(d, bench_dir, c)
            bench_args = create_args(bench_data.data_path)

            commands.match_features.Command().run(bench_args)
            commands.create_tracks.Command().run(bench_args)
            commands.reconstruct.Command().run(bench_args)

            res = benchmark(bench_data, c)

            bench_result['datasets'].append(d)
            bench_result['results'].append(res)

        bench_results.append(bench_result)

    for br in bench_results:
        logger.info("========================================")
        run_definition = "Report: "
        for key, value in br["config"].items():
            run_definition += key + ": " + str(value) + ", "

        logger.info(run_definition)
        logger.info("Dataset:")
        logger.info("\t".join(br["datasets"]))
        logger.info("Score:")
        logger.info("\t".join([str(r['score']) for r in br["results"]]))
        logger.info("Reprojection error (OpenSfM coordinates):")
        logger.info("\t".join([str(r['reprojection_error']) for r in br["results"]]))
        logger.info("Reprojection error (pixels):")
        logger.info("\t".join([str(r['reprojection_error_pixel']) for r in br["results"]]))
        logger.info("Reconstructed tracks (length >= 3):")
        logger.info("\t".join([str(r['tracks_3']) for r in br["results"]]))
        logger.info("========================================")
